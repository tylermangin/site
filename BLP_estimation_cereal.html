<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Tyler Mangin" />


<title>BLP Estimation on Cereal Data</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}

.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Tyler Mangin</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Projects
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="research.html">Research</a>
    </li>
    <li>
      <a href="statistics.html">Statistics</a>
    </li>
    <li>
      <a href="Teaching.html">Teaching</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:tyler.mangin@gmail.com">
    <span class="fa fa-envelope-o fa-2x"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/tylermangin">
    <span class="fa fa-github fa-2x"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/RealTylerMangin">
    <span class="fa fa-twitter fa-2x"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/tyler-mangin-72779197/">
    <span class="fa fa-linkedin fa-2x"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">BLP Estimation on Cereal Data</h1>
<h4 class="author"><em>Tyler Mangin</em></h4>

</div>


<p>The original code was written for Matlab by <a href="http://faculty.wcas.northwestern.edu/~ane686/supplements/rc_dc_code.htm">Aviv Nevo</a> in 1998. <a href="https://github.com/mcarniol/Berry-Levinsohn-and-Pakes-1995-in-R">Michael Carniol</a> adapted it into R in 2015, to which I owe the functions and set-up I use here. I just became aware of another R package designed to estimate BLP, <a href="https://cran.r-project.org/web/packages/BLPestimatoR/BLPestimatoR.pdf">BLPestimatoR</a>, which I have not had a chance to test.</p>
<p>I ran this routine on a canonical data set from Aviv Nevo. I found the data here: <a href="http://www.rasmusen.org/zg604/lectures/blp/" class="uri">http://www.rasmusen.org/zg604/lectures/blp/</a>. A lot of the description is due to <a href="http://faculty.wcas.northwestern.edu/~ane686/research/RAs_guide.pdf">Nevo’s Practitioner’s Guide</a>. Any errors are mine.</p>
<div id="setup" class="section level3">
<h3>Setup</h3>
<p>The data elements we need are:</p>
<ol style="list-style-type: decimal">
<li><p>The share of unit sales of several products</p></li>
<li><p>Several markets</p></li>
<li><p>A set of product characteristics (including price)</p></li>
</ol>
<p>In these data, the share has been constructed, the products are indicated by the “brand” field, the markets are cities, and the product characteristics are price, the “mushiness” of the cereal, and the amount of sugar in the cereal.</p>
<pre class="r"><code>  while(!require(AER)){install.packages(&quot;AER&quot;)}</code></pre>
<pre><code>## Warning: package &#39;AER&#39; was built under R version 3.5.1</code></pre>
<pre class="r"><code>  while(!require(SQUAREM)){install.packages(&quot;SQUAREM&quot;)}
  while(!require(BB)){install.packages(&quot;BB&quot;)}</code></pre>
<pre><code>## Warning: package &#39;BB&#39; was built under R version 3.5.1</code></pre>
<pre class="r"><code>  while(!require(plyr)){install.packages(&quot;plyr&quot;)}


  share.fld =     &quot;share&quot;
  prod.id.fld =   &quot;brand&quot;
  mkt.id.fld =    &quot;city&quot;
  prc.fld =       &quot;price&quot;
  x.var.flds =    c(&quot;sugar&quot;,
                    &quot;mushy&quot;)
  
   
  #Set up data
  dat &lt;- dat[dat[, share.fld] &gt; 0, ]
  dat &lt;- dat[order(dat[, mkt.id.fld], dat[, prod.id.fld]), ]
  JM &lt;- nrow(dat)</code></pre>
<p>Let the indirect utility (maximized utility) of consumer <span class="math inline">\(i\)</span>, with income <span class="math inline">\(y_i\)</span>, of product <span class="math inline">\(j\)</span> in market <span class="math inline">\(m\)</span> be</p>
<p><span class="math display">\[u_{ijm} = \alpha_i(y_i - p_{jm}) + x_{jm}\beta_i + \xi_{jm} + \epsilon_{ijm} \]</span></p>
<p>where <span class="math inline">\(x_{jm}\)</span> is a set of observed product/market characteristics, <span class="math inline">\(\xi_{jm}\)</span> is a set of unobserved product/market characteristics, and <span class="math inline">\(\epsilon_{ijm}\)</span> is a mean-zero stochastic term.</p>
<p>This equation is written for maximum explanatory power, but is not as useful in thinking about estimation. For instance, <span class="math inline">\(p_{jm}\)</span> is separated out in the above equation, but in practice it can be included in <span class="math inline">\(x_{jm}\)</span>, which can cause more confusion than you would think. I will include <span class="math inline">\(p_{jm}\)</span> in the <span class="math inline">\(nxk\)</span> matrix <span class="math inline">\(x\)</span> from here on out, partly by defining <span class="math inline">\(\theta_{i} = [\alpha_{i}, \beta_{i}]\)</span>, which becomes useful later.</p>
<pre class="r"><code>  #Number of characteristics (including constant and price)
  X &lt;- as.matrix(cbind(ones = rep(1, JM), dat[, c(x.var.flds, prc.fld)]));
  K &lt;- ncol(X)</code></pre>
<p>Additionally, although income <span class="math inline">\(y_i\)</span> is important for theory, it drops out of the estimation (exactly at which step is indicated below), so I will exclude that from here on out as well.</p>
<p>That makes my simplified, less pedagogic version of the above :</p>
<p><span class="math display">\[u_{ijm} = x_{jm}\theta_{i} + \xi_{jm} + \epsilon_{ijm} \]</span> (Equation 1)</p>
</div>
<div id="defining-markets-and-market-shares" class="section level3">
<h3>Defining Markets and Market Shares</h3>
<p>These data are not at an individual level, so we will express demand in terms of market share. In this data set, most of the process has been done, so that we are given the set of shares.</p>
<p>Observed market shares are defined in theory as <span class="math inline">\(s_{jm} = q_{jm}/M\)</span>. Technically, <span class="math inline">\(\sum_J q_{jm} = M\)</span> but, the practice usually used is to define a market <span class="math inline">\(M\)</span> based on some population measure. Almost surely, the sum of observed <span class="math inline">\(q\)</span> across all the choices is smaller than <span class="math inline">\(M\)</span>, which creates take a set of <span class="math inline">\(s_{jm}\)</span> that do not sum to 1 across <span class="math inline">\(j\)</span>.</p>
<pre class="r"><code>  #market object
  mkt.id &lt;- dat[, mkt.id.fld];

  #shares object
  s.jm &lt;- as.vector(dat[, share.fld]);

  #sum of produt shares by market
  temp &lt;- aggregate(s.jm, by = list(mkt.id = mkt.id), sum);

  summary(temp)</code></pre>
<pre><code>##      mkt.id            x         
##  Min.   : 1.00   Min.   :0.1847  
##  1st Qu.:17.00   1st Qu.:0.4096  
##  Median :33.00   Median :0.4601  
##  Mean   :32.34   Mean   :0.4700  
##  3rd Qu.:46.00   3rd Qu.:0.5501  
##  Max.   :65.00   Max.   :0.6728</code></pre>
<p>So, in this case, the observed unit sales constituted about 40-55% of the “market”, however it was defined. This makes it possible to define an “outside good” <span class="math inline">\(s_{m0}\)</span> as the difference. Having an ‘outside good’ is not just useful for creating shares, but also solves an identification problem in the final estimation: the estimation can only identify <span class="math inline">\(j-1\)</span> parameters, so the ‘outside good’ acts as the reference point for the rest of the goods in the choice set.</p>
<pre class="r"><code>  #Compute the outside good market share by market

  sum1 &lt;- temp$x[match(mkt.id, temp$mkt.id)];
  s.j0 &lt;- as.vector(1 - sum1);
  rm(temp, sum1);</code></pre>
</div>
<div id="models" class="section level2">
<h2>Models</h2>
<p>How the model is estimated depends crucially on what assumptions are made on how <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> vary across individuals.</p>
<div id="the-logit-model" class="section level3">
<h3>The Logit Model</h3>
<p>If it is assumed that all consumer heterogeneity is captured in <span class="math inline">\(\epsilon_{ijm}\)</span>, then equation 1 becomes:</p>
<p><span class="math display">\[u_{ijm} = x_{jm}\theta + \xi_{jm} + \epsilon_{ijm} \]</span></p>
<p>Importantly, that means that <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are the same across all consumers, so <span class="math inline">\(\theta_{i}\)</span> is now just <span class="math inline">\(\theta = [\alpha, \beta]\)</span>, which does not vary across <span class="math inline">\(i\)</span>.</p>
<p>If it is additionally assumed that <span class="math inline">\(\epsilon_{ijm}\)</span> are i.i.d Type I extreme value distributions, then the shares have a simple closed form:</p>
<p><span class="math display">\[ s_{jm} = \frac{ exp( x_{jm}\theta + \xi_{jm} + \epsilon_{ijm} )}{1 + \sum_{k \neq j} exp( x_{jm}\theta + \xi_{jm} + \epsilon_{ijm} ) } \]</span></p>
<p>In this and all following formulations, if the the term <span class="math inline">\(\alpha_i y_i\)</span> is included from the beginning, it drops out here since it does not vary across options.</p>
<p>These assumptions make <span class="math inline">\(s_{jm}\)</span> log-linear:</p>
<p><span class="math display">\[ \delta_{jm} = ln(s_{jm}) - log(s_{m0}) = x_{jm}\theta_{i} + \xi_{jm} + \epsilon_{ijm} \]</span></p>
<p>In no way have we solved the simultaneity of <span class="math inline">\(p_{jm}\)</span>, or dealt with the unobserved variable bias of <span class="math inline">\(\xi_{jm}\)</span>, both of which need to be addressed with based on the data available and the situation.</p>
<p>But, the nice thing about this form is that by transforming a non-linear problem into a linear problem, we can use straightforward linear estimation.</p>
<pre class="r"><code>  # delta object
  dat[, &quot;delta&quot;] &lt;- Y &lt;- log(s.jm) - log(s.j0);

  #OLS
  fm.olsreg = paste0(&quot;delta ~ &quot;, 
                          paste(x.var.flds, collapse = &quot; + &quot;), &quot; + &quot;, 
                          paste(prc.fld, collapse = &quot; + &quot;))


  ols = lm(data = dat,
           formula = fm.olsreg)
  
  summary(ols)</code></pre>
<pre><code>## 
## Call:
## lm(formula = fm.olsreg, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7943 -0.7224  0.0055  0.7570  3.3952 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -2.988385   0.153176 -19.509  &lt; 2e-16 ***
## sugar         0.049561   0.006014   8.241 4.73e-16 ***
## mushy         0.059955   0.071458   0.839    0.402    
## price       -10.428699   1.201357  -8.681  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.128 on 1124 degrees of freedom
## Multiple R-squared:  0.09366,    Adjusted R-squared:  0.09125 
## F-statistic: 38.72 on 3 and 1124 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Since we linearized the problem, 2SLS is also possible. In this data set, instruments are included (<span class="math inline">\(z_1, z_2, z_3\)</span>). I’m not sure what they are.</p>
<p>We are also going to carefully save many of the objects from the 2SLS regression for use later.</p>
<pre class="r"><code>#2SLS
  
  prc.iv.flds = c(&quot;z1&quot;,
                  &quot;z2&quot;,
                  &quot;z3&quot;)

  beta.est = NULL;

  str.ivreg.y &lt;- &quot;delta ~ &quot;
  str.ivreg.x &lt;- paste(x.var.flds, collapse = &quot; + &quot;)
  str.ivreg.prc &lt;- paste(prc.fld, collapse = &quot; + &quot;)
  str.ivreg.iv &lt;- paste(prc.iv.flds, collapse = &quot; + &quot;)
  print(&quot;2SLS specification:&quot;)</code></pre>
<pre><code>## [1] &quot;2SLS specification:&quot;</code></pre>
<pre class="r"><code>  print(fm.ivreg &lt;- paste0(str.ivreg.y, str.ivreg.x, &quot; + &quot;, str.ivreg.prc, &quot; | &quot;, str.ivreg.x, &quot; + &quot;, str.ivreg.iv))</code></pre>
<pre><code>## [1] &quot;delta ~ sugar + mushy + price | sugar + mushy + z1 + z2 + z3&quot;</code></pre>
<pre class="r"><code>  rm(str.ivreg.y, str.ivreg.x, str.ivreg.prc, str.ivreg.iv)
  
  print(&quot;2SLS beta estimate:&quot;)</code></pre>
<pre><code>## [1] &quot;2SLS beta estimate:&quot;</code></pre>
<pre class="r"><code>  print(summary(mo.ivreg &lt;- ivreg(fm.ivreg, data = dat, x = TRUE)))</code></pre>
<pre><code>## 
## Call:
## ivreg(formula = fm.ivreg, data = dat, x = TRUE)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -4.06082 -0.74808 -0.02715  0.76105  3.51798 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -1.697126   1.406373  -1.207 0.227786    
## sugar         0.064219   0.017046   3.767 0.000174 ***
## mushy         0.008283   0.092873   0.089 0.928951    
## price       -21.594304  12.147217  -1.778 0.075721 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.17 on 1124 degrees of freedom
## Multiple R-Squared: 0.02401, Adjusted R-squared: 0.02141 
## Wald test: 13.68 on 3 and 1124 DF,  p-value: 9.036e-09</code></pre>
<pre class="r"><code>  beta.est &lt;- summary(mo.ivreg)$coef[, 1:2]
  #Z = instrumental variable matrix include exogenous X&#39;s
  Z &lt;- as.matrix(mo.ivreg$x$instruments)
  PZ &lt;- Z %*% solve(t(Z) %*% Z) %*% t(Z);
  theta1 &lt;- coef(mo.ivreg);
  xi.hat &lt;- as.vector(mo.ivreg$resid);
  Z.hat &lt;- Z * matrix(rep(xi.hat, ncol(Z)), ncol = ncol(Z))
  W.inv &lt;- try(solve(t(Z.hat) %*% Z.hat), silent = FALSE)
  if(&quot;matrix&quot; == class(W.inv)){
    PZ &lt;- Z %*% W.inv %*% t(Z);
    PX.inv &lt;- solve(t(X) %*% PZ %*% X)
    theta1 &lt;- PX.inv %*% t(X) %*% PZ %*% Y
    xi.hat &lt;- Y - X %*% theta1
    X.hat &lt;- (PZ %*% X) * matrix(rep(xi.hat, K), ncol = K)
    tsls.se &lt;- sqrt(diag(PX.inv %*% t(X.hat) %*% X.hat %*% PX.inv))
    # print(&quot;GMM step 2 updated theta1 estimate:&quot;)
    # print(beta.est &lt;- data.frame(beta.est = theta1, se.est = tsls.se))
  }
  dat[, &quot;xi.hat&quot;] &lt;- xi.hat</code></pre>
</div>
<div id="random-coefficents" class="section level3">
<h3>Random Coefficents</h3>
<p>If <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\beta_i\)</span> are individual-specific parameters, we need to model their distribution. A common way of doing this is to assume <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\beta_i\)</span> follow a multivariate normal. If <span class="math inline">\(v_i\)</span> is an individual’s deviation from the average, then this assumption amounts to</p>
<p><span class="math display">\[ \begin{bmatrix} \alpha_i \\ \beta_i \end{bmatrix}  \sim N(\begin{bmatrix} \alpha \\ \beta \end{bmatrix},\Sigma v_i)\]</span></p>
<p>In aggregate market data, consumers are not observed, and so the <span class="math inline">\(v_i\)</span> are simulated.</p>
<pre class="r"><code>## Matrix of individuals&#39; characteristics ##
  #number of simulated consumers
  n.sim = 100
  
  #Standard normal distribution draws, one for each characteristic in X
    #columns are simulated consumers, rows are variables in X (including constant and price)
  v = matrix(rnorm(K * n.sim), nrow = K, ncol = n.sim)</code></pre>
<p>This assumption introduces some non linearity, so linear estimation is no longer an option. However, there still are linear portions, so it will be useful to separate out the linear and non-linear parts going forward. Our simplified equation can be written in the following useful form</p>
<p><span class="math display">\[u_{ijm} = \delta_{jm} +  \mu_{ijm}\]</span></p>
<p>1.) Linear Terms <span class="math inline">\(\delta_{jm}\)</span></p>
<p>Represents the mean average value to all consumers.</p>
<p>The linear parameters are <span class="math inline">\(\theta^1 = [\alpha, \beta]\)</span>, corresponding to the average marginal utility of money and of product characteristics.</p>
<p><span class="math display">\[\delta_{jm} = x_{jm}\theta^1 + \xi_{jm} + \epsilon_{ijm}\]</span></p>
<p>2.) Nonlinear Terms <span class="math inline">\(\mu_{ijm}\)</span></p>
<p>Represents the mean-zero deviation from <span class="math inline">\(\delta_{jm}\)</span>.</p>
<p>The the nonlinear parameter are <span class="math inline">\(\theta^2 = [\Sigma]\)</span>, corresponding to individual deviations from marginal utility of money and of product characteristics.</p>
<p><span class="math display">\[\mu_{ijm} =  x_{jm}\Sigma v_i \]</span></p>
<p>The market share of the <span class="math inline">\(j\)</span>th product is the integral over the population distribution <span class="math inline">\(P^*(v,\epsilon)\)</span> of consumers in market <span class="math inline">\(M\)</span> which bought product <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[ s_{jm}(x_m,p_m,\delta_m; \theta_2) = \int_{m_{j}} dP^*(v,\epsilon) \]</span></p>
<p>Let <span class="math inline">\(S\)</span> be the actual observed shares in the data, as opposed to the model’s shares <span class="math inline">\(s\)</span>. The estimation algorithm becomes</p>
<p><span class="math display">\[ Min_{\delta_m,\theta_2} || s_{jm}(x_m,p_m;\delta_m, \theta_2) - S  || \]</span></p>
</div>
</div>
<div id="optimization-routine" class="section level2">
<h2>Optimization routine</h2>
<p>To run the optimization, we will use the multiStart function from the BB package. One feature of this function is that starts the optimization from multiple starting points, to test the sensitivity of the results to starting values.</p>
<pre class="r"><code>  multiStart(par,
             fn,
             gr,
             action = c(&quot;solve&quot;, &quot;optimize&quot;),
             method=c(2,3,1),
             lower=-Inf,
             upper=Inf,
             project=NULL,
             projectArgs=NULL,
             control=list(),
             quiet=FALSE,
             details=FALSE)</code></pre>
<p>The first input is the parameter vector we are optimizing, which in our case is values of <span class="math inline">\(\theta^2\)</span>. The second input is a non-linear objective function to be optimized. In our case this will be a function that takes the <span class="math inline">\(\theta^2\)</span> vector as an input and returns the value of the objective function <span class="math inline">\(f\)</span> ( gmm_obj, defined below). Next, multiStart takes a gradient input, which is another function taking the <span class="math inline">\(\theta^2\)</span> parameter vector as an input and returning the gradient vector of the objective function at that point.</p>
<p>Now, we will set up the functions that run the optimization. We will start at the innermost point in the function and work out. The optimization runs in steps, in which iteratively different estimates of the parameters are used to calculate the objective function, which we are hoping to minimize.</p>
<p>The main reason the BLP algorithm works is the contraction mapping on the “inner loop” of the optimization. The contraction mapping takes as an input the estimated market shares using the whatever the estimated parameters are in the current step. It uses a formula analogous to the closed form expression from above, in which the market shares are the average over the <span class="math inline">\(n\)</span> observations for the simulated consumers.</p>
<p><span class="math display">\[ s_{jm}(x_m,p_m,\delta_m; \theta_2)  =  \frac{1}{r*n} \Sigma_{r*n} s_{jm} =  \frac{1}{r*n} \Sigma_{r*n}  \frac{ exp( x_{jm}\theta_1 + \xi_{jm} + \epsilon_{ijm} )}{1 + \sum_{k \neq j} exp( x_{jm}\theta + \xi_{jm} + \epsilon_{ijm} ) } \]</span></p>
<pre class="r"><code>  ind_sh &lt;- function(delta.in, mu.in){
    # This function computes the &quot;individual&quot; probabilities of choosing each brand
    # Requires global variables: mkt.id, X, v
    numer &lt;- exp(mu.in) * matrix(rep(exp(delta.in), n.sim), ncol = n.sim);
    denom &lt;- as.matrix(do.call(&quot;rbind&quot;, lapply(mkt.id, function(tt){
      1 + colSums(numer[mkt.id %in% tt, ])
    })))
    return(numer / denom);  
  }</code></pre>
<p>With that as an input, the contraction mapping consists of updating a step <span class="math inline">\(h\)</span> estimate of of mean utility <span class="math inline">\(\delta^h\)</span> with the difference between the log observed shares <span class="math inline">\(S\)</span> and the log predicted shares <span class="math inline">\(s\)</span></p>
<p><span class="math display">\[ \delta^{h+1} = \delta^h + ln(S) - ln(s(x_m,p_m,\delta_m; \theta_2)) \]</span></p>
<pre class="r"><code>  blp_inner &lt;- function(delta.in, mu.in) {
    # Computes a single update of the BLP (1995) contraction mapping.
    # of market level predicted shares.
    # This single-update function is required by SQUAREM, see Varadhan and
    # Roland (SJS, 2008), and Roland and Varadhan (ANM, 2005)
    # INPUT
    #   delta.in : current value of delta vector
    #   mu.in: current mu matrix
    # Requires global variables: s.jm
    # OUTPUT
    #   delta.out : delta vector that equates observed with predicted market shares
    pred.s &lt;- rowMeans(ind_sh(delta.in, mu.in));
    delta.out &lt;- delta.in + log(s.jm) - log(pred.s)
    return(delta.out)
  }</code></pre>
<p>Additionally, we will be accelerating the convergence of this contraction mapping using the squarem function from the SQUAREM package. The function takes a fixed-point function (in our case blp_inner), which accepts <span class="math inline">\(\delta^h\)</span> and returns <span class="math inline">\(\delta^{h+1}\)</span>.</p>
<p>To move from the inner loop to the objective function, we use the updated estimate <span class="math inline">\(\hat{\delta}\)</span> to form an estimate of the individual utility of the unobserved product characteristics <span class="math inline">\(\hat{\xi} = \hat{\delta} - x_{jm}\theta^1\)</span>. From that, we get the method of moments condition</p>
<p><span class="math display">\[E[\xi_i&#39;Z(Z&#39;\xi_i&#39;\xi_i&#39;Z)^{-1}Z&#39;\xi_i] = 0\]</span></p>
<p>And, with an estimate <span class="math inline">\(\hat{\xi_i}\)</span>, and using <span class="math inline">\(Z&#39;Z\)</span> as a consistent estimate <span class="math inline">\(E[Z&#39;\xi_i&#39;\xi_i&#39;Z]\)</span>, the above is our objective function to minimize.</p>
<pre class="r"><code>  gmm_obj &lt;- function(theta2){
    # This function computes the GMM objective function
    # Requires global variable inputs: X, v, delta, a, W
    # Outputs: theta1, xi.hat
    print(paste0(&quot;GMM Loop number: &quot;, Sys.time()))
    print(a &lt;&lt;- a + 1)
    print(&quot;Updated theta2 estimate:&quot;)
    print(theta2)
    print(&quot;Change in theta2 estimate:&quot;)
    print(theta.chg &lt;- as.numeric(theta2 - theta2.prev));
    if(sum(theta.chg != 0) &lt;= 2){
      delta &lt;- dat[, &quot;delta&quot;];
    } else {
      delta &lt;- Y;
    }
    theta2.prev &lt;&lt;- theta2;
    
    mu &lt;- X %*% diag(theta2) %*% v;
    
    print(&quot;Running SQUAREM contraction mapping&quot;)
    print(system.time(
      squarem.output &lt;- squarem(par = delta, fixptfn = blp_inner, mu.in = mu, control = list(trace = TRUE))
    ));
    delta &lt;- squarem.output$par
    print(summary(dat[, &quot;delta&quot;] - delta));
    dat[, &quot;delta&quot;] &lt;&lt;- delta;
    
    mo.ivreg &lt;- ivreg(fm.ivreg, data = dat, x = TRUE)
    theta1 &lt;&lt;- coef(mo.ivreg);
    xi.hat &lt;&lt;- as.vector(mo.ivreg$resid);
    Z.hat &lt;- Z * matrix(rep(xi.hat, ncol(Z)), ncol = ncol(Z))
    W.inv &lt;- try(solve(t(Z.hat) %*% Z.hat), silent = FALSE)

      if(&quot;matrix&quot; == class(W.inv)){

      PX.inv &lt;- solve(t(X) %*% PZ %*% X)
      theta1 &lt;&lt;- PX.inv %*% t(X) %*% PZ %*% delta
      xi.hat &lt;&lt;- delta - X %*% theta1
      X.hat &lt;- (PZ %*% X) * matrix(rep(xi.hat, K), ncol = K)
      tsls.se &lt;- sqrt(diag(PX.inv %*% t(X.hat) %*% X.hat %*% PX.inv))
      print(&quot;GMM step 2 updated theta1 estimate:&quot;)
      print(beta.est &lt;&lt;- data.frame(beta.est = theta1, beta.se = tsls.se, sigma.est = theta2))
      print(&quot;made it here&quot;)
    }

        dat[, &quot;xi.hat&quot;] &lt;&lt;- xi.hat
    f &lt;- t(xi.hat) %*% Z %*% W.inv %*% t(Z) %*% xi.hat;
    
    
    print(&quot;Updated GMM objective:&quot;)
    print(f &lt;- as.numeric(f));
    return(f)
  }</code></pre>
<p>In general, we could just use the objective function, systematically varying the estimated parameters to look for the minimized value. However, since our model is smooth, and has derivatives, we should be able to use that information to make better choices about how to vary our parameter estimates. This is called the Quasi-Newton’s Method, and consists of constructing a linear approximation of the objective function around the current estimate (using the current step’s estimated parameters).</p>
<p>This method takes two inputs: first, it requires the jacobian of the objective function.</p>
<pre class="r"><code>  jacobian &lt;- function(delta.in, theta.in){
    print(paste0(&quot;Calculating Jacobian matrix, &quot;, Sys.time()))
    #Requires global variables X, v, mkt.id
    mu1 &lt;- X %*% diag(theta.in) %*% v;
    ind.shares &lt;- ind_sh(delta.in, mu1);
    K &lt;- ncol(X);
    print(paste0(&quot;Calculating dsigma matrix, &quot;, Sys.time()))
    dsigma &lt;- lapply(l.Xv, function(x){
      temp2 &lt;- x * ind.shares;
      temp3 &lt;- as.matrix(do.call(&quot;rbind&quot;, lapply(mkt.id, function(m){
        colSums(temp2[mkt.id %in% m, ])
      })));
      dsigma.res &lt;- rowMeans(temp2 - ind.shares * temp3);
      return(dsigma.res)
    })
    dsigma &lt;- as.matrix(do.call(&quot;cbind&quot;, dsigma))
    print(paste0(&quot;Calculating ddelta matrices, &quot;, Sys.time()))
    ddelta &lt;- list()
    for(m in mkt.id){
      if(m %in% names(ddelta)){next}
      temp1 &lt;- as.matrix(ind.shares[mkt.id %in% m, ]);
      H1 &lt;- temp1 %*% t(temp1);
      H2 &lt;- diag(rowSums(temp1));
      H &lt;- (H2 - H1) / n.sim;
      H.inv &lt;- solve(H);
      ddelta[[as.character(m)]] &lt;- H.inv %*% dsigma[mkt.id %in% m, ];
      rm(temp1, H1, H2, H, H.inv)
    }
    ddelta &lt;- as.matrix(do.call(&quot;rbind&quot;, ddelta));
    return(ddelta)
  }</code></pre>
<p>Second, it requires the gradient, evaluated at the mean utility level predicted by the parameters inside of step <span class="math inline">\(h\)</span>.</p>
<pre class="r"><code>  gradient_obj &lt;- function(theta2){
    #Requires global variables PZ, delta, xi.hat
    print(system.time(jacobian_res &lt;&lt;- jacobian(as.vector(dat[, &quot;delta&quot;]), theta2)))
    print(paste0(&quot;Updated gradient:&quot;, Sys.time()))
    print(f &lt;- -2 * as.numeric(t(jacobian_res) %*% PZ %*% xi.hat));
    #######
    L &lt;- ncol(Z)
    covg &lt;- matrix(0, nrow = L, ncol = L)
    for(i in 1:JM){
      covg &lt;- covg + (Z[i, ] %*% t(Z[i, ])) * xi.hat[i]^2
    }
    d.delta &lt;- jacobian_res;
    Dg &lt;- t(d.delta) %*% Z
    p.Dg &lt;- try(solve(Dg %*% W.inv %*% t(Dg)))
    cov.mat &lt;- p.Dg %*% (Dg %*% W.inv %*% covg %*% W.inv %*% t(Dg)) %*% p.Dg
    beta.est$sigma.se &lt;&lt;- sqrt(diag(cov.mat));
    print(paste0(&quot;Updated coefficients table:&quot;, Sys.time()))
    print(beta.est)
    write.csv(beta.est, file = paste0(&quot;BLP_beta_est_&quot;, Sys.Date(), &quot;.csv&quot;))
    #######
    return(as.numeric(f))
  }</code></pre>
<p>So, with the functions all set up, all we will need to finalize the procedure are initial values for the parameters. One option, of course, is to guess. But we can potentially do better. In the logit we already estimated the standard errors for the linear parameters <span class="math inline">\(\theta^1\)</span>. Since we already estimated it, we can use that as our guess.</p>
<pre class="r"><code>  #Starting point
  print(&quot;Sigma guess:&quot;)</code></pre>
<pre><code>## [1] &quot;Sigma guess:&quot;</code></pre>
<pre class="r"><code>    # tsls.se = beta.est[, 2]
    print(theta2 &lt;- 0.5 * tsls.se);</code></pre>
<pre><code>##        ones       sugar       mushy       price 
## 0.727581577 0.008837817 0.049185287 6.303916821</code></pre>
<pre class="r"><code>    theta2 = t(theta2)
    
  theta2.prev &lt;- theta2;</code></pre>
<p>With an initial value guess, we can put it all together by passing multistart the parameter vector we are optimizing, the objective function to be optimized. and the gradient.</p>
<p>At that point, we need only sit back and hope for convergence.</p>
<pre class="r"><code>    # Break X and v matrices into list variables 
  # in attempt to expedite calculation of the Jacobian matrix
  l.X &lt;- lapply(1:K, function(k){
    return(X[, k])
  })
  l.v &lt;- lapply(1:K, function(k){
    return(v[k, ])
  })
  l.Xv &lt;- lapply(1:K, function(k){
    l.X[[k]] %*% t(l.v[[k]]);
  })
  
  print(&quot;Estimating random coefficients multinomial logit&quot;)
  a &lt;- 0;
  beta.est &lt;- NULL;
  print(system.time(
    theta.est &lt;- multiStart(par = theta2, fn = gmm_obj, gr = gradient_obj, lower = 0, control = list(trace = TRUE), action = &quot;optimize&quot;)
  ));
  save(theta.est, file = paste0(&quot;theta_est_.RData&quot;))</code></pre>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
